{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR0_MKOOeRNO",
        "outputId": "3dedc32c-7ff8-471a-99d8-6b2e61c91dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers -U -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RtEvp3Me4va",
        "outputId": "c576a0c3-1f18-4688-ad66-d1891c0befbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "! pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLwUoKhxD-jt",
        "outputId": "718a5c84-4c6a-4a8e-a21a-a25730b4d059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformers==4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip freeze | grep transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF2Q_NNQF7o0",
        "outputId": "976475ba-73b8-43cb-fa84-5933e689a6e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt8imwfi0FzS",
        "outputId": "130de581-e689-41ab-e4c3-93ad07caf1ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mqJLyp4eZaj",
        "outputId": "89695f51-6e45-463b-f7d9-a645031c6251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd \n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\")\n",
        "\n",
        "\n",
        "st.title('Project final')\n",
        "st.header('Translate Application')\n",
        "st.text('Team member')\n",
        "df = pd.DataFrame({\"ID\": [\"20127102\", \"20127364\", \"20127395\",\"20127444\", \"20127448\"],\n",
        "                    \"Name\": [\"Hoang Huu Minh An\", \"Nguyen Vo Minh Tri\", \"Phan Minh Xuan\", \"Bui Duy Bao\", \"Nguyen Thai Bao\"]})\n",
        "st.table(df)\n",
        "\n",
        "\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\")\n",
        "\n",
        "option=('Arabic' ,'Czech', 'German', 'Spanish', 'Estonian','Vietnamese', 'Finnish','French', 'Hindi', 'Italian', 'Japanese', 'Korean', 'Lithuanian', 'Nepali', 'Russian', 'Chinese', 'Afrikaans', 'Azerbaijani', 'Bengali', 'Croatian', 'Indonesian', 'Malayalam', 'Thai', 'Tagalog', 'Ukrainian')\n",
        "\n",
        "st.title(\"Language Translator\")\n",
        "\n",
        "\n",
        "with st.form(\"my_form\"):\n",
        "    \n",
        "    article = st.text_area(\"Enter text:\",height=None,max_chars=None,key=None,help=\"Enter your text here\")\n",
        "    tokenizer(article, return_tensors=\"pt\")\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "    selected_option_translate = st.selectbox('From English Translate To', options= option)\n",
        "    if selected_option_translate == \"French\":\n",
        "      translation_FR = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Vietnamese\":\n",
        "      translation_VN = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"vi_VN\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Arabic\":\n",
        "      translation_AR = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"ar_AR\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Czech\":\n",
        "      translation_CZ = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"cs_CZ\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"German\":\n",
        "      translation_GE = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"de_DE\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Spanish\":\n",
        "      translation_SP = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"es_XX\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Estonian \":\n",
        "      translation_ES = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"et_EE\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Finnish\":\n",
        "      translation_FI = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"fi_FI\"]), skip_special_tokens=True)\n",
        "    \n",
        "    elif selected_option_translate == \"Hindi\":\n",
        "      translation_HI = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Italian\":\n",
        "      translation_IT = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"it_IT\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Japanese\":\n",
        "      translation_JA = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"ja_XX\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Korean\":\n",
        "      translation_KO = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"ko_KR\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Lithuanian\":\n",
        "      translation_LI = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"lt_LT\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Nepali\":\n",
        "      translation_NE = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"ne_NP\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Russian\":\n",
        "      translation_RU = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"]), skip_special_tokens=True)\n",
        "\n",
        "    elif selected_option_translate == \"Chinese\":\n",
        "      translation_CH = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Afrikaans\":\n",
        "      translation_AF = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"af_ZA\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Azerbaijani\":\n",
        "      translation_AZ = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"az_AZ\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Bengali\":\n",
        "      translation_BE = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"bn_IN\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Croatian\":\n",
        "      translation_CR = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"hr_HR\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Indonesian\":\n",
        "      translation_IN = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"id_ID\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Malayalam\":\n",
        "      translation_MA = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"ml_IN\"]), skip_special_tokens=True)\n",
        "\n",
        "    elif selected_option_translate == \"Thai\":\n",
        "      translation_Thai = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"th_TH\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Tagalog\":\n",
        "      translation_TA = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"tl_XX\"]), skip_special_tokens=True)\n",
        "    elif selected_option_translate == \"Ukrainian\":\n",
        "      translation_UK = tokenizer.batch_decode(model.generate(**tokenizer(article, return_tensors=\"pt\"), forced_bos_token_id=tokenizer.lang_code_to_id[\"uk_UA\"]), skip_special_tokens=True)\n",
        "    \n",
        "    submitted = st.form_submit_button(\"Translate Sentence\")\n",
        "    if submitted:\n",
        "        if selected_option_translate == \"French\":\n",
        "          st.text_input(\"Text Translation: \" , translation_FR[0])\n",
        "        elif selected_option_translate == \"Vietnamese\":\n",
        "          st.text_input(\"Text Translation: \" , translation_VN[0])\n",
        "        elif selected_option_translate == \"Arabic\":\n",
        "          st.text_input(\"Text Translation: \" , translation_AR[0])\n",
        "        elif selected_option_translate == \"Czech\":\n",
        "          st.text_input(\"Text Translation: \" , translation_CZ[0])\n",
        "        elif selected_option_translate == \"German\":\n",
        "          st.text_input(\"Text Translation: \" , translation_GE[0])\n",
        "        elif selected_option_translate == \"Spanish\":\n",
        "          st.text_input(\"Text Translation: \" , translation_SP[0])\n",
        "        elif selected_option_translate == \"Estonian\":\n",
        "          st.text_input(\"Text Translation: \" , translation_ES[0])\n",
        "        elif selected_option_translate == \"Finnish\":\n",
        "          st.text_input(\"Text Translation: \" , translation_FI[0])\n",
        "\n",
        "        elif selected_option_translate == \"Hindi\":\n",
        "          st.text_input(\"Text Translation: \" , translation_HI[0])\n",
        "        elif selected_option_translate == \"Italian\":\n",
        "          st.text_input(\"Text Translation: \" , translation_IT[0])\n",
        "        elif selected_option_translate == \"Japanese\":\n",
        "          st.text_input(\"Text Translation: \" , translation_JA[0])\n",
        "        elif selected_option_translate == \"Korean\":\n",
        "          st.text_input(\"Text Translation: \" , translation_KO[0])\n",
        "\n",
        "        elif selected_option_translate == \"Lithuanian\":\n",
        "          st.text_input(\"Text Translation: \" , translation_LI[0])\n",
        "        elif selected_option_translate == \"Nepali\":\n",
        "          st.text_input(\"Text Translation: \" , translation_NE[0])\n",
        "        elif selected_option_translate == \"Russian\":\n",
        "          st.text_input(\"Text Translation: \" , translation_RU[0])\n",
        "        \n",
        "        elif selected_option_translate == \"Chinese\":\n",
        "          st.text_input(\"Text Translation: \" , translation_CH[0])\n",
        "        elif selected_option_translate == \"Afrikaans\":\n",
        "          st.text_input(\"Text Translation: \" , translation_AF[0])\n",
        "        elif selected_option_translate == \"Azerbaijani\":\n",
        "          st.text_input(\"Text Translation: \" , translation_AZ[0])\n",
        "        elif selected_option_translate == \"Bengali\":\n",
        "          st.text_input(\"Text Translation: \" , translation_BE[0])\n",
        "        elif selected_option_translate == \"Croatian\":\n",
        "          st.text_input(\"Text Translation: \" , translation_CR[0])\n",
        "        elif selected_option_translate == \"Indonesian\":\n",
        "          st.text_input(\"Text Translation: \" , translation_IN[0])\n",
        "        elif selected_option_translate == \"Malayalam\":\n",
        "          st.text_input(\"Text Translation: \" , translation_MA[0])\n",
        "\n",
        "        elif selected_option_translate == \"Thai\":\n",
        "          st.text_input(\"Text Translation: \" , translation_Thai[0])\n",
        "        elif selected_option_translate == \"Tagalog\":\n",
        "          st.text_input(\"Text Translation: \" , translation_TA[0])\n",
        "        elif selected_option_translate == \"Ukrainian\":\n",
        "          st.text_input(\"Text Translation: \" , translation_UK[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzj93gO7f0Gc"
      },
      "outputs": [],
      "source": [
        "# model_inputs = tokenizer(article_en, return_tensors=\"pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiD6pDFC4QxQ",
        "outputId": "ee68d132-dbac-4b39-90db-f158cbf1d936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=803d7159388ba53c624294753f8d7987d88635fec133934d882392d230832d72\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ],
      "source": [
        "! pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCg0UA8X4XOT"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2Q3YYCW3FFx"
      },
      "outputs": [],
      "source": [
        "# !nohup streamlit run app.py &\n",
        "# url = ngrok.connect(port = '8051')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXwLbdWM6xum",
        "outputId": "21ccb00f-e68a-4398-91ce-5ed543f68c23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 6.086s\n",
            "your url is: https://giant-lies-wear.loca.lt\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.197.7.147:8501\u001b[0m\n",
            "\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "2023-05-19 16:31:29.468660: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUvZnzn68Lz7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}